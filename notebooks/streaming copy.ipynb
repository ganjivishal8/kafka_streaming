{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_version = \"2.12\"\n",
    "spark_version = \"3.5.0\"\n",
    "\n",
    "packages = f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}'\n",
    "spark = SparkSession.builder. \\\n",
    "                    appName(\"KafkaSparkConnect\"). \\\n",
    "                    config(\"spark.jars.packages\", packages). \\\n",
    "                    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOT_STRAP = \"localhost:9092\"\n",
    "TOPIC = \"kafkademo1\"\n",
    "\n",
    "kafka_df = spark.readStream. \\\n",
    "                format(\"kafka\"). \\\n",
    "                option(\"kafka.bootstrap.servers\", BOOT_STRAP). \\\n",
    "                option(\"subscribe\", TOPIC). \\\n",
    "                load()\\\n",
    "                .withColumn(\"value\", regexp_replace(col(\"value\").cast(\"string\"), \"\\\\\\\\\", \"\")) \\\n",
    "                .withColumn(\"value\", regexp_replace(col(\"value\"), \"^\\\"|\\\"$\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_df = kafka_df. \\\n",
    "#         selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# base_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_schema = (\n",
    "        StructType()\n",
    "        .add(\"id\",StringType())\n",
    "        .add(\"first_name\", StringType())\n",
    "        .add(\"last_name\", StringType())\n",
    "        .add(\"company\", StringType())\n",
    "        .add(\"role\", StringType())\n",
    "    )\n",
    "\n",
    "\n",
    "fin_df = kafka_df.select(\n",
    "        from_json(col(\"value\"), sample_schema).alias(\"sample\")\n",
    "    ).select(\"sample.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, first_name: string, last_name: string, company: string, role: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_df.withColumn(\"id\",col(\"id\").cast(\"int\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fin_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"jdbc:postgresql://localhost:5432/scd_2\"\n",
    "# source_name='source_scd'\n",
    "# dest_name='dest_scd'\n",
    "username='postgres'\n",
    "password='Vish@08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df,id):\n",
    "    df.write.mode(\"append\") \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"driver\",\"org.postgresql.Driver\") \\\n",
    "      .option(\"url\", url) \\\n",
    "      .option(\"dbtable\", \"kafka_stream\") \\\n",
    "      .option(\"user\", username) \\\n",
    "      .option(\"password\", password) \\\n",
    "      .save()\n",
    "    pass\n",
    "# \n",
    "fin_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(foreach_batch_function)\\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ROUGH PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = fin_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"C:/Users/SpringMl/kafka/my_output\") \\\n",
    "    .option(\"checkpointLocation\", \"C:/Users/SpringMl/kafka/my_check\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.streaming import Trigger\n",
    "# import org.apache.spark.sql.streaming.Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df.writeStream.format(\"jdbc\"). \\\n",
    "                    option(\"url\", url). \\\n",
    "                    option(\"driver\", \"org.postgresql.Driver\"). \\\n",
    "                    option(\"dbtable\", \"kafka_stream\"). \\\n",
    "                    option(\"user\", username). \\\n",
    "                    option(\"password\", password). \\\n",
    "                    outputMode(\"append\").trigger(Trigger.ProcessingTime(\"1 minute\")).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\n",
    "{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PASEO COSTA DEL SUR\",\"State\":\"PR\"}\n",
    "{\"Zipcode\":709,\"ZipCodeType\":\"STANDARD\",\"City\":\"BDA SAN LUIS\",\"State\":\"PR\"}\n",
    "{\"Zipcode\":76166,\"ZipCodeType\":\"UNIQUE\",\"City\":\"CINGULAR WIRELESS\",\"State\":\"TX\"}\n",
    "{\"Zipcode\":76177,\"ZipCodeType\":\"STANDARD\",\"City\":\"FORT WORTH\",\"State\":\"TX\"}\n",
    "{\"Zipcode\":76177,\"ZipCodeType\":\"STANDARD\",\"City\":\"FT WORTH\",\"State\":\"TX\"}\n",
    "{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"URB EUGENE RICE\",\"State\":\"PR\"}\n",
    "{\"Zipcode\":85209,\"ZipCodeType\":\"STANDARD\",\"City\":\"MESA\",\"State\":\"AZ\"}\n",
    "{\"Zipcode\":85210,\"ZipCodeType\":\"STANDARD\",\"City\":\"MESA\",\"State\":\"AZ\"}\n",
    "{\"Zipcode\":32046,\"ZipCodeType\":\"STANDARD\",\"City\":\"HILLIARD\",\"State\":\"FL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
